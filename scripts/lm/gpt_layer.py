# Copyright 2024 InstaDeep Ltd. All rights reserved.#

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# the RoPE implementation was directly taken from hugging face code :
# transformers/models/gptj/modeling_flax_gptj.py
# These rotary positional embeddings are proper to GPT-like implementation ( LLAMA and
# GPTJ in trix ). dimensions in key space are rotated 2 by 2. The key difference with
# ESM's one is that in this case these groups of 2 dimensions are adjacent ( cf. main
# figure of the Roformer paper : Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020).
# Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451. )

import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import haiku as hk
import jax
import jax.numpy as jnp
import numpy as np
from haiku import initializers

from scripts.lm.lm_types import AttentionMask, Embedding
from scripts.lm.utils import debug_log_tensor, get_activation_fn

logger = logging.getLogger(__name__)


class SimpleLMHead(hk.Module):
    """
    Basic Language Model head. Transforms final attention block output
    into a distribution over tokens at each sequence position.
    """

    def __init__(
        self,
        embed_dim: int,
        alphabet_size: int,
        add_bias_lm_head: bool = True,
        name: Optional[str] = None,
    ):
        """
        Args:
            embed_dim: Embedding dimension.
            alphabet_size: Number of tokens in the alphabet.
            name: Name of the layer. Defaults to None.
        """
        super().__init__(name=name)
        self.embed_dim = embed_dim
        self.alphabet_size = alphabet_size

        # Define layers
        w_init = initializers.VarianceScaling(2.0, "fan_in", "uniform")
        b_init = initializers.VarianceScaling(2.0, "fan_in", "uniform")
        self._final_fc = hk.Linear(
            self.alphabet_size,
            w_init=w_init,
            b_init=b_init,
            with_bias=add_bias_lm_head,
            name="lm_final_fc",
        )

    def __call__(self, x: jnp.ndarray) -> Dict[str, jnp.ndarray]:
        # Compute logits
        logits = self._final_fc(x)
        return {"logits": logits}


def create_sinusoidal_positions(num_pos: int, dim: int) -> np.ndarray:
    """
    Create the sinus and cosines for the RoPE

    Args:
        num_pos: the number of position to encode
        dim: the dimension of the RoPE

    Returns:
        Array of size (num_pos, 2*dim) containing the sinus and cosinus for RoPE
    """

    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))
    sinusoid_inp = np.einsum("i , j -> i j", np.arange(num_pos), inv_freq)
    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)

    sentinel = dim // 2 + dim % 2
    jmp_policy = hk.mixed_precision.current_policy()
    if jmp_policy is None:
        # default float32
        compute_dtype = np.float32
    else:
        # cast to jmp policy if specified
        compute_dtype = jmp_policy.compute_dtype

    sincos = np.zeros((num_pos, dim), dtype=compute_dtype)
    sincos[:, 0:sentinel] = sin
    sincos[:, sentinel:] = cos

    return np.array(sincos)


def rotate_every_two(attention_tensor: jnp.ndarray) -> jnp.ndarray:
    """
    Prepare a tensor to apply the RoPE mechanism

    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor

    Returns:
        The even indices in the last dimension have their sign flipped
        tensor size : (batch_size, seq_len, num_heads, key_dim)
    """
    rotate_half_tensor = jnp.stack(
        (-attention_tensor[:, :, :, 1::2], attention_tensor[:, :, :, ::2]), axis=-1
    )
    rotate_half_tensor = rotate_half_tensor.reshape(
        rotate_half_tensor.shape[:-2] + (-1,)
    )
    return rotate_half_tensor


def apply_rotary_pos_emb(
    attention_tensor: jnp.ndarray, sincos: jnp.ndarray
) -> jnp.ndarray:
    """
    Apply the RoPE to attention_tensor
    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor
        sincos: the sincos generated by the function 'create_sinusoidal_positions'
            shape :

    Returns:
        the corresponding RoPE-encoded tensor
    """
    sin_pos, cos_pos = sincos
    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
    return (attention_tensor * cos_pos) + (rotate_every_two(attention_tensor) * sin_pos)


@dataclass
class RotaryEmbeddingConfig:
    """
    Rotary Positional Embedding configuration
        max_seq_len: The number of positions to encode and cache.
        dim: Dimension of RoPE.
        theta: Rotation angle.
    """

    max_seq_len: int
    dim: int
    theta: float


class RotaryEmbedding(hk.Module):
    """
    Rotary Positional Embedding inspired by GPT-like models (LLAMA and GPTJ).
    """

    def __init__(
        self,
        config: RotaryEmbeddingConfig,
        name: Optional[str] = None,
    ):
        """
        Args:
            config: Rotary Positional Embedding configuration.
            name: Name of the layer. Defaults to None.
        """
        super().__init__(name=name)
        self.max_seq_len = config.max_seq_len
        self.dim = config.dim
        self.theta = config.theta
        self.sincos_cache = self._create_sinusoidal_positions()

    def _create_sinusoidal_positions(self) -> np.ndarray:
        """
        Create the sines and cosines for the RoPE.

        Returns:
            Sinusoidal positions of shape (self.max_seq_len, self.dim).
        """
        inv_freq = 1.0 / (self.theta ** (np.arange(0, self.dim, 2) / self.dim))
        sinusoid_inp = np.einsum("i , j -> i j", np.arange(self.max_seq_len), inv_freq)
        sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)
        sentinel = self.dim // 2 + self.dim % 2
        jmp_policy = hk.mixed_precision.current_policy()
        if jmp_policy is None:
            compute_dtype = np.float32
        else:
            compute_dtype = jmp_policy.compute_dtype
        sincos = np.zeros((self.max_seq_len, self.dim), dtype=compute_dtype)
        sincos[:, 0:sentinel] = sin
        sincos[:, sentinel:] = cos
        return np.array(sincos)

    def _rotate_every_two(self, x: jnp.ndarray) -> jnp.ndarray:
        """
        Prepare a tensor to apply the RoPE mechanism.

        Args:
            x: Tensor of shape (batch_size, seq_len, num_heads, head_dim),
               typically this is the key or query tensor.

        Returns:
            The even indices in the last dimension have their sign flipped.
            Tensor of shape (batch_size, seq_len, num_heads, head_dim).
        """
        rotate_half = jnp.stack((-x[:, :, :, 1::2], x[:, :, :, ::2]), axis=-1)
        rotate_half = rotate_half.reshape(rotate_half.shape[:-2] + (-1,))
        return rotate_half

    def _apply_rotary_pos_emb(self, x: jnp.ndarray, sincos: jnp.ndarray) -> jnp.ndarray:
        """
        Applies rotary embeddings to x.

        Args:
            x: Tensor of shape (batch_size, seq_len, num_heads, head_dim),
               typically this is the key or query tensor.
        Returns:
            Rope embeddings tensor.
        """
        sin_pos, cos_pos = sincos
        sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
        cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
        return (x * cos_pos) + (self._rotate_every_two(x) * sin_pos)

    def __call__(
        self, k: jnp.ndarray, q: jnp.ndarray, positions: Optional[jnp.ndarray] = None
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Applies rotary embeddings to k and q.

        Args:
            k: key tensor of shape (batch_size, seq_len, num_heads, head_dim),
            q: value tensor of shape (batch_size, seq_len, num_heads, head_dim),
            positions: optional positions offset useful when caching,

        Returns:
            RoPE embeddings for the keys and values.
        """
        position_ids = jnp.arange(0, k.shape[1], 1, dtype=jnp.int32)
        position_ids = jnp.expand_dims(position_ids, 0).repeat(k.shape[0], 0)
        if positions is not None:
            position_ids += positions
        sincos = jnp.take(self.sincos_cache, position_ids, axis=0)
        sincos = jnp.split(sincos, 2, axis=-1)
        k_rot = self._apply_rotary_pos_emb(k[:, :, :, : self.dim], sincos)
        k_pass = k[:, :, :, self.dim :]
        q_rot = self._apply_rotary_pos_emb(q[:, :, :, : self.dim], sincos)
        q_pass = q[:, :, :, self.dim :]
        keys = jnp.concatenate([k_rot, k_pass], axis=-1)
        values = jnp.concatenate([q_rot, q_pass], axis=-1)
        return keys, values


class GptMultiHeadAttention(hk.Module):
    """
    Multi-head attention with masking applied. Modified from Haiku implementation to
    be able to support relative positional embeddings.  Computes the keys, queries, and
    values from the input embeddings.  Future versions could compute and store these
    to prevent redundant calculations during autoregressive inference. The keys, queries
    , and value sizes are fixed to be embed_dim/num_heads in accordance with the
    standard Gpt model.
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        rotary_dim: Optional[int],
        max_position_embeddings: int,
        key_size: Optional[int] = None,
        name: Optional[str] = "attention",
        add_bias_attn: bool = False,
        dropout_rate: float = 0.2,
    ):
        """
        Initializes the attention layer.

        Args:
            embed_dim: Length of the token embedding at each position in the sequence.
            num_heads: Number of independent attention heads.
            rotary_dim: The dimension of the rotary positional embedding in each key
                space
            max_position_embeddings: the maximum positions used for the computation of
                the RoPE
            key_size: dimension of the key vectors
            name: Optional name for this module.
            add_bias_attn: Add bias to the attention mechanism (key, query, value, and
                output projections).
        """
        super().__init__(name=name)
        self.num_heads = num_heads
        self.max_position_embeddings = max_position_embeddings
        self.embed_dim = embed_dim
        self.key_size = key_size or self.embed_dim // self.num_heads
        self.rotary_dim = rotary_dim or self.key_size
        self.add_bias_attn = add_bias_attn

        self.key_linear = hk.Linear(
            output_size=self.key_size * num_heads,
            with_bias=self.add_bias_attn,
            name="key_linear",
        )
        self.query_linear = hk.Linear(
            output_size=self.key_size * num_heads,
            with_bias=self.add_bias_attn,
            name="query_linear",
        )
        self.value_linear = hk.Linear(
            output_size=self.key_size * num_heads,
            with_bias=self.add_bias_attn,
            name="value_linear",
        )

        self.out_linear = hk.Linear(
            output_size=embed_dim,
            with_bias=self.add_bias_attn,
            name="out_linear",
        )

        self.dropout_rate = dropout_rate

        self.sincos_positions = None

    def get_sincos_positions(self) -> jnp.ndarray:
        """
        Generate the sincos_positions tensor
        Returns:
            array of shape (max_position_embeddings, rotary_dim) containing the sinus
            and cosinus for the RoPE embedding
        """
        if self.sincos_positions is None:
            self.sincos_positions = create_sinusoidal_positions(
                self.max_position_embeddings, self.rotary_dim
            )
        return self.sincos_positions

    def __call__(
        self,
        query_inputs: jnp.ndarray,
        key_inputs: jnp.ndarray,
        value_inputs: jnp.ndarray,
        attention_mask: Optional[jnp.ndarray],
    ) -> jnp.ndarray:
        """
        Computes the result of multiheaded dot-product attention, using
        pre-computed projections for the queries, keys, and values.

        Args:
            query_inputs: Embeddings that will be projected to become the queries.
            key_inputs: Embeddings that will be projected to become the keys.
            value_inputs: Embeddings that will be projected to become the values.
            attention_mask: Mask to be applied in the attention layers.
                Triangular for autoregressive models.
                shape : (1, 1, seq_len, seq_len)

        Returns:
            The standard output of multi-headed attention
        """
        position_ids = jnp.arange(0, key_inputs.shape[1], 1, dtype=jnp.int32)
        position_ids = jnp.expand_dims(position_ids, 0).repeat(key_inputs.shape[0], 0)

        keys = self.key_linear(key_inputs)
        queries = self.query_linear(query_inputs)
        values = self.value_linear(value_inputs)

        # debug printing
        debug_log_tensor("MHA keys", keys, logger=logger)
        debug_log_tensor("MHA queries", queries, logger=logger)
        debug_log_tensor("MHA values", values, logger=logger)

        keys = keys.reshape(keys.shape[0], keys.shape[1], self.num_heads, -1)
        queries = queries.reshape(queries.shape[0], queries.shape[1], self.num_heads, -1)
        values = values.reshape(values.shape[0], values.shape[1], self.num_heads, -1)

        sincos = jnp.take(self.get_sincos_positions(), position_ids, axis=0)
        sincos = jnp.split(sincos, 2, axis=-1)

        if self.rotary_dim is not None:
            k_rot = keys[:, :, :, : self.rotary_dim]
            k_pass = keys[:, :, :, self.rotary_dim :]

            q_rot = queries[:, :, :, : self.rotary_dim]
            q_pass = queries[:, :, :, self.rotary_dim :]

            k_rot = apply_rotary_pos_emb(k_rot, sincos)
            q_rot = apply_rotary_pos_emb(q_rot, sincos)

            keys = jnp.concatenate([k_rot, k_pass], axis=-1)
            queries = jnp.concatenate([q_rot, q_pass], axis=-1)
        else:
            keys = apply_rotary_pos_emb(keys, sincos)
            queries = apply_rotary_pos_emb(queries, sincos)

        attention_logits = jnp.einsum("...thd,...Thd->...htT", queries, keys)
        sqrt_key_size = jnp.sqrt(keys.shape[-1]).astype(queries.dtype)
        attention_logits = attention_logits / sqrt_key_size

        attention_logits = jnp.where(attention_mask, attention_logits, -1e30)

        attention_weights = jax.nn.softmax(attention_logits, axis=-1)

        if self.dropout_rate > 0:
            random_key = hk.next_rng_key()
            attention_weights = hk.dropout(
                random_key, self.dropout_rate, attention_weights
            )

        values = jnp.einsum("...htT,...Thd->...thd", attention_weights, values)
        values = jnp.reshape(values, (values.shape[0], values.shape[1], -1))

        # debug printing
        debug_log_tensor("MHA final keys", keys, logger=logger)
        debug_log_tensor("MHA final queries", queries, logger=logger)
        debug_log_tensor("MHA attention logits", attention_logits, logger=logger)
        debug_log_tensor("MHA attention weights", attention_weights, logger=logger)
        debug_log_tensor("MHA final values", values, logger=logger)

        return self.out_linear(values)


class GptDecoderLayer(hk.Module):
    """
    Single layer in the encoder, including self-attention and feed-forward operations.
    The feed-forward network uses a ReLU activation and has no biases.
    """

    def __init__(
        self,
        embed_dim: int,
        ffn_embed_dim: int,
        num_heads: int,
        rotary_dim: Optional[int],
        max_position_embeddings: int,
        norm_type: str,
        parallel_attention_ff: bool,
        add_bias_ffn: bool,
        ffn_activation_name: str,
        use_glu_in_ffn: bool,
        add_bias_attn: bool = False,
        dropout_rate: float = 0.2,
        name: Optional[str] = None,
    ):
        """
        Initializes the encoder layer, including the projections needed for
        self-attention and the linear layers applied in the fully connected portion

        Args:
            embed_dim: Dimension of the embeddings
            ffn_embed_dim: Dimension of the hidden layer in the MLP
            num_heads: Number of independent attention heads.
            rotary_dim: The dimension in key space to apply the rotary positional
                embeddings
            max_position_embeddings: The maximum length to apply rotary positional
                embeddings
            norm_type: The type of norm used ( pre normalization scheme ) used. can be
                one of ["layer_norm", "RMS_norm"]
            parallel_attention_ff: Whether to do the attention and the MLP in parallel,
                and then sum up the results as it is done in Gpt-NeoX :
                Black, Sid, et al. "Gpt-neox-20b: An open-source autoregressive
                language model." arXiv preprint arXiv:2204.06745 (2022).
                It is said to improve the training time of 15% when compiling with JAX
            add_bias_ffn: Add bias in feed forward network block.
            ffn_activation_name: Activation function to be used in FFN block. Supported
                names are "gelu", "gelu-no-approx", "relu", "swish", and "silu"
            use_glu_in_ffn: Whether to use Gated Linear Unit (GLU) in Feed
                Forward Network (FFN) block. To do a swiGLU (gated-swish) put this arg
                to True and use swish as ffn_activation_name.
                Same principle for a gated-relu.
            name: Optional name for this module.
            add_bias_attn: Add bias to the attention mechanism (key, query, value, and
                output projections).
        """
        super().__init__(name=name)

        self.num_heads = num_heads
        self.parallel_attention_ff = parallel_attention_ff
        self.sa_layer = GptMultiHeadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            name="self_attn",
            rotary_dim=rotary_dim,
            max_position_embeddings=max_position_embeddings,
            add_bias_attn=add_bias_attn,
            dropout_rate=dropout_rate,
        )

        if norm_type == "layer_norm":
            self.attn_norm = hk.LayerNorm(
                axis=-1, create_scale=True, create_offset=True, name="attn_layer_norm"
            )
            if not (self.parallel_attention_ff):
                self.ffn_norm = hk.LayerNorm(
                    axis=-1,
                    create_scale=True,
                    create_offset=True,
                    name="ffn_layer_norm",
                )
        elif norm_type == "RMS_norm":
            self.attn_norm = hk.RMSNorm(
                axis=-1, create_scale=True, name="attn_RMS_norm", eps=1e-6
            )
            if not (self.parallel_attention_ff):
                self.ffn_norm = hk.RMSNorm(
                    axis=-1, create_scale=True, name="ffn_RMS_norm", eps=1e-6
                )
        else:
            raise ValueError(f"unrecognized norm_type : {norm_type}")

        # Get ffn activation function
        self._ffn_activation_fn = get_activation_fn(activation_name=ffn_activation_name)
        self._use_glu_in_fnn = use_glu_in_ffn

        # Define layers
        if use_glu_in_ffn:
            # user should multiply ffn_embed_dim by 2/3 when using GLU
            # to keep total number of parameters equal
            # see https://arxiv.org/pdf/2002.05202.pdf. for more details
            # we multiply by 2 here as the output will be split in 2 for GLU
            ffn_embed_dim = int(2 * ffn_embed_dim)

        self.fc1_linear = hk.Linear(
            output_size=ffn_embed_dim,
            with_bias=add_bias_ffn,
            name="fc1_linear_glu" if use_glu_in_ffn else "fc1_linear",
        )
        self.fc2_linear = hk.Linear(
            output_size=embed_dim, with_bias=add_bias_ffn, name="fc2_linear"
        )

        self.dropout_rate = dropout_rate

    @hk.transparent
    def mlp(self, x: Embedding) -> Embedding:
        """
        Applies one linear layer, a ReLU activation, dropout, then a final linear layer.

        Args:
            x: Embeddings of shape (batch_size, seq_len, embed_dim).

        Returns:
            The transformed sequence embedding.
        """
        if self._use_glu_in_fnn:
            x1, x2 = jnp.split(self.fc1_linear(x), indices_or_sections=2, axis=-1)
            x = self._ffn_activation_fn(x1) * x2
        else:
            x = self._ffn_activation_fn(self.fc1_linear(x))

        x = self.fc2_linear(x)
        return x

    def __call__(
        self,
        embeddings: Embedding,
        attention_mask: AttentionMask,
    ) -> Embedding:
        """
        Computes the output embeddings of the encoder layer.
        if self.parallel_attention_ff, the model uses parallel MLP and attention

        Args:
            embeddings: Decoder layer input embeddings of shape
                (batch_size,seq_len,embed_dim).
            attention_mask: Mask to be applied in the attention layers.
                Triangular for autoregressive models.
                shape = (1, 1, seq_len, seq_len)

        Returns:
            The output embeddings that result from the application of the layer
        """
        if self.parallel_attention_ff:
            residuals = embeddings

            embeddings = self.attn_norm(embeddings)
            attn_outputs = self.sa_layer(
                query_inputs=embeddings,
                key_inputs=embeddings,
                value_inputs=embeddings,
                attention_mask=attention_mask,
            )
            mlp_ouputs = self.mlp(embeddings)
            return residuals + attn_outputs + mlp_ouputs

        else:
            normed_embeddings = self.attn_norm(embeddings)
            attn_outputs = embeddings + self.sa_layer(
                query_inputs=normed_embeddings,
                key_inputs=normed_embeddings,
                value_inputs=normed_embeddings,
                attention_mask=attention_mask,
            )
            mlp_outputs = attn_outputs + self.mlp(self.ffn_norm(attn_outputs))

        if self.dropout_rate > 0:
            random_key = hk.next_rng_key()
            mlp_outputs = hk.dropout(random_key, self.dropout_rate, mlp_outputs)

        return mlp_outputs
